+++
audioDuration = "00:42:45"
audioFile = "Google.Cloud.Platform.Podcast.Episode.114.mp3"
audioSize = 61517512
categories = ["Machine Learning", "Machine Learning Bias", "Machine Learning Fairness", "AI Ethics"]
date = "2018-02-14"
description = "Timnit Gebru and Margaret Mitchell discuss machine learning bias and fairness."
draft = false
episodeNumber = 114
hosts = ["Mark Mandel", "Melanie Warrick"]
title = "Machine Learning Bias and Fairness with Timnit Gebru and Margaret Mitchell"
linked = true
googlePlusLink = "https://plus.google.com/u/0/b/117267610519909886476/+Gcppodcast/posts/8BEHuq5uJs9"
redditLink = "https://www.reddit.com/r/gcppodcast/comments/7xj7ld/episode_114_machine_learning_bias_and_fairness/"
+++

This week, we dive into machine learning bias and fairness from a social and technical perspective with machine learning research scientists [Timnit Gebru](https://github.com/timnitgebru) from Microsoft and [Margaret Mitchell](https://github.com/mmitchell_ai) (aka Meg, aka M.) from Google. 

They share with [Melanie](https://twitter.com/nyghtowl) and [Mark](https://twitter.com/Neurotic) about ongoing efforts and resources to address bias and fairness including diversifying datasets, applying algorithmic techniques and expanding research team expertise and perspectives. There is not a simple solution to the challenge, and they give insights on what work in the broader community is in progress and where it is going.   

<!--more-->

##### Timnit Gebru
[Timnit Gebru](https://www.microsoft.com/en-us/research/people/tigebru/) works in the Fairness Accountability Transparency and Ethics (FATE) group at the New York Lab. Prior to joining Microsoft Research, she was a PhD student in the Stanford Artificial Intelligence Laboratory, studying computer vision under Fei-Fei Li. Her main research interest is in data mining large-scale, publicly available images to gain sociological insight, and working on computer vision problems that arise as a result, including fine-grained image recognition, scalable annotation of images, and domain adaptation. The Economist and others have recently covered part of this work. She is currently studying how to take dataset bias into account while designing machine learning algorithms, and the ethical considerations underlying any data mining project. As a cofounder of the group Black in AI, she works to both increase diversity in the field and reduce the impact of racial bias in the data.

##### Margaret Mitchell
[M. Mitchell](http://www.m-mitchell.com/) is a Senior Research Scientist in Google’s Research & Machine Intelligence group, working on artificial intelligence. Her research involves vision-language and grounded language generation, focusing on how to evolve artificial intelligence toward positive goals. Margaret’s work combines machine learning, computer vision, natural language processing, social media, and insights from cognitive science. Before Google, Margaret was a founding member of Microsoft Research’s “Cognition” group, focused on advancing artificial intelligence, and a researcher in Microsoft Research’s Natural Language Processing group.

##### Cool things of the week
- GPS/Cellular Asset Tracking using Google Cloud IoT Core, Firestore and MongooseOS [blog](https://medium.com/google-cloud/gps-cellular-asset-tracking-using-google-cloud-iot-core-firestore-and-mongooseos-4dd74921f582) 
- GPUs in Kubernetes Engine now available in beta [blog](https://cloudplatform.googleblog.com/2018/02/accelerate-highly-parallelized-compute-tasks-with-GPUs-in-Kubernetes-Engine.html) 
- Announcing Spring Cloud GCP - integrating your favorite Java framework with Google Cloud [blog](https://cloudplatform.googleblog.com/2018/02/announcing-Spring-Cloud-GCP-integrating-your-favorite-Java-framework-with-Google-Cloud.html)  

##### Interview

- PAIR | People+AI Research Initiative [site](https://ai.google/pair)
- FATE | Fairness, Accountability, Transparency and Ethics in AI [site](https://www.microsoft.com/en-us/research/group/fate/)
- Fat* Conference [site](https://fatconference.org/) & [resources](https://fatconference.org/links.html)
- Joy Buolamwini [site](https://www.media.mit.edu/people/joyab/overview/) 
- Algorithmic Justice Leaguge [site](https://www.ajlunited.org/)
- ProPublica Machine Bias [article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
- AI Ethics & Society Conference [site](http://www.aies-conference.com/)
- Ethics in NLP Conference [site](http://www.ethicsinnlp.org/)
- FACETS [site](https://pair-code.github.io/facets/)
- TensorFlow Lattice [repo](https://github.com/tensorflow/lattice)

Sample papers on bias and fairness:

- Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification [paper](http://proceedings.mlr.press/v81/buolamwini18a.html)
- Facial Recognition is Accurate, if You're a White Guy [article](https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html)
- Mitigating Unwanted Biases with Adversarial Learning [paper](https://arxiv.org/pdf/1801.07593.pdf) 
- Improving Smiling Detection with Race and Gender Diversity [paper](https://arxiv.org/pdf/1712.00193.pdf)
- Fairness Through Awareness [paper](https://arxiv.org/pdf/1104.3913.pdf)
- Avoiding Discrimination through Casual Reasoning [paper](https://arxiv.org/pdf/1706.02744.pdf)
- Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings [paper](https://arxiv.org/pdf/1607.06520.pdf)
- Satisfying Real-world Goals with Dataset Constraints [paper](https://papers.nips.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints.pdf)
- Axiomatic Attribution for Deep Networks [paper](https://seejane.org/research-informs-empowers/data/)
- Monotonic Calibrated Interpolated Look-Up Tables [paper](http://www.jmlr.org/papers/volume17/15-243/15-243.pdf)
- Equality of Opportunity in Machine Learning [blog](https://research.googleblog.com/2016/10/equality-of-opportunity-in-machine.html)

Additional links:

- Bill Nye Saves the World Episode 3: Machines Take Over the World (includes Margaret Mitchell) [site](https://www.netflix.com/title/80117748)
- "We're in a diversity crisis": Black in AI's founder on what's poisoning the algorithms in our lives [article](https://www.technologyreview.com/s/610192/were-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our/)
- Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru [TWiML & AI podcast](https://twimlai.com/twiml-talk-88-using-deep-learning-google-street-view-estimate-demographics-timnit-gebru/)
- Security and Safety in AI: Adversarial Examples, Bias and Trust with Mustapha Cisse [TWiML & AI podcast](https://twimlai.com/twiml-talk-108-security-safety-ai-adversarial-examples-bias-trust-moustapha-cisse/)

##### Question of the week

"Is there a gcp service that's cloud identity-aware proxy except for a static site that you host via cloud storage?" 

- [Answer](https://twitter.com/kf/status/963195282434408448) between [Mark](https://twitter.com/Neurotic) & [KF](https://github.com/kf)
- Cloud Identity-Aware Proxy [site](https://cloud.google.com/iap/) & [docs](https://cloud.google.com/iap/docs/)
- Cloud Storage [site](https://cloud.google.com/storage/) & [docs](https://cloud.google.com/storage/docs/)
- Hosting a Static Website on Cloud Storage [site](https://cloud.google.com/storage/docs/hosting-static-website)
- Google App Engine [site](https://cloud.google.com/appengine/) & [docs](https://cloud.google.com/appengine/docs/)
- weasel [repo](https://github.com/google/weasel)

##### Where can you find us next?

Melanie will be at [Fat*](https://fatconference.org/) in New York in Feb.

Mark will be at the [Game Developer's Conference | GDC](http://www.gdconf.com/) in March.


